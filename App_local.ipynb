{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2353ec46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "from flask import Flask, request, jsonify\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Initialize Flask app\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "# Load the query classifier\n",
    "zero_shot_classifier = pipeline(\"zero-shot-classification\", model=\"valhalla/distilbart-mnli-12-1\")\n",
    "query_labels = [\"chitchat\", \"wiki_query\"]\n",
    "query_hypothesis_template = \"This is a {}.\"\n",
    "\n",
    "# Load the chitchat model\n",
    "blenderbot_tokenizer = AutoTokenizer.from_pretrained(\"facebook/blenderbot-400M-distill\")\n",
    "blenderbot_model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/blenderbot-400M-distill\")\n",
    "\n",
    "# Load the T5 model for summarization\n",
    "t5_model_name = \"t5-base\"\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(t5_model_name)\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(t5_model_name)\n",
    "\n",
    "# Reload the zero-shot classification model and tokenizer for topic classification\n",
    "load_directory = \"./saved_zero_shot_model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(load_directory)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(load_directory)\n",
    "pipe = pipeline(\"zero-shot-classification\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Define topics and hypothesis template for topic classification\n",
    "topics = [\n",
    "    \"Health\",\n",
    "    \"Environment\",\n",
    "    \"Technology\",\n",
    "    \"Economy\",\n",
    "    \"Entertainment\",\n",
    "    \"Sports\",\n",
    "    \"Politics\",\n",
    "    \"Education\",\n",
    "    \"Travel\",\n",
    "    \"Food\",\n",
    "]\n",
    "topic_hypothesis_template = \"This query is related to {}.\"\n",
    "leaving_remarks = [\"bye\", \"goodbye\", \"exit\", \"see you\", \"later\", \"quit\"]  # Define leaving remarks\n",
    "\n",
    "# Utility functions\n",
    "def classify_query_type(query, classifier, labels, threshold=0.5):\n",
    "    output = classifier(query, labels, hypothesis_template=query_hypothesis_template)\n",
    "    scores = output[\"scores\"]\n",
    "    best_label = output[\"labels\"][0]\n",
    "    best_score = scores[0]\n",
    "    return {\"label\": best_label, \"score\": best_score} if best_score > threshold else {\"label\": \"uncertain\", \"score\": best_score}\n",
    "\n",
    "def chat_with_blenderbot(input_text):\n",
    "    inputs = blenderbot_tokenizer(input_text, return_tensors=\"pt\")\n",
    "    outputs = blenderbot_model.generate(**inputs)\n",
    "    return blenderbot_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "def classify_multi_topics(query, pipe, topics, threshold=0.1, top_n=5):\n",
    "    output = pipe(query, topics, hypothesis_template=topic_hypothesis_template)\n",
    "    labels = output[\"labels\"]\n",
    "    scores = output[\"scores\"]\n",
    "    return [label for label, score in zip(labels, scores) if score > threshold][:top_n]\n",
    "\n",
    "def preprocess_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    words = word_tokenize(text.lower())\n",
    "    filtered_words = [\n",
    "        lemmatizer.lemmatize(word) for word in words if word.isalnum() and word not in stop_words\n",
    "    ]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "def load_preprocessed_data(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def create_tfidf_vectorizer(texts):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    return vectorizer, tfidf_matrix\n",
    "\n",
    "def get_most_relevant_articles(query, tfidf_vectorizer, tfidf_matrix, articles, top_n=3):\n",
    "    query_vec = tfidf_vectorizer.transform([query])\n",
    "    cosine_similarities = np.dot(tfidf_matrix, query_vec.T).toarray().flatten()\n",
    "    sorted_indices = cosine_similarities.argsort()[::-1]\n",
    "    unique_articles = []\n",
    "    seen_urls = set()\n",
    "    for idx in sorted_indices:\n",
    "        if len(unique_articles) >= top_n:\n",
    "            break\n",
    "        article = articles[idx]\n",
    "        article_url = article.get('url', None)\n",
    "        if article_url and article_url not in seen_urls:\n",
    "            unique_articles.append(article)\n",
    "            seen_urls.add(article_url)\n",
    "    return unique_articles\n",
    "\n",
    "def generate_meaningful_summary_t5(combined_text, max_length=800, min_length=100):\n",
    "    inputs = t5_tokenizer.encode(\"summarize: \" + combined_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    summary_ids = t5_model.generate(\n",
    "        inputs,\n",
    "        max_length=max_length,\n",
    "        min_length=min_length,\n",
    "        length_penalty=1.0,\n",
    "        num_beams=4,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    return t5_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "def combine_summaries(articles):\n",
    "    combined_text = \" \".join([\n",
    "        article['summary']['text_en'] if 'summary' in article and 'text_en' in article['summary'] else \"No Summary\"\n",
    "        for article in articles\n",
    "    ])\n",
    "    if not combined_text.strip():\n",
    "        return \"No valid summaries available to generate a meaningful summary.\"\n",
    "    return generate_meaningful_summary_t5(combined_text)\n",
    "\n",
    "def wiki_qa_system(query, preprocessed_data, top_n=3):\n",
    "    relevant_topics = classify_multi_topics(query, pipe, topics, threshold=0.1, top_n=5)\n",
    "    all_articles = []\n",
    "    for topic, data in preprocessed_data.items():\n",
    "        if topic in relevant_topics and 'articles' in data and isinstance(data['articles'], list):\n",
    "            for article in data['articles']:\n",
    "                if isinstance(article, dict) and 'preprocessed_summary' in article:\n",
    "                    article['topic'] = topic\n",
    "                    all_articles.append(article)\n",
    "    if not all_articles:\n",
    "        raise ValueError(\"No valid articles found for the relevant topics.\")\n",
    "\n",
    "    articles_summaries = [\n",
    "        preprocess_text(article['preprocessed_summary']) for article in all_articles\n",
    "        if 'preprocessed_summary' in article\n",
    "    ]\n",
    "    if not articles_summaries:\n",
    "        raise ValueError(\"No preprocessed summaries found for matching.\")\n",
    "\n",
    "    tfidf_vectorizer, tfidf_matrix = create_tfidf_vectorizer(articles_summaries)\n",
    "    most_relevant_articles = get_most_relevant_articles(query, tfidf_vectorizer, tfidf_matrix, all_articles, top_n)\n",
    "    combined_summary = combine_summaries(most_relevant_articles)\n",
    "    \n",
    "    answers = [{\n",
    "        'title': article.get('title', \"No Title\"),\n",
    "        'topic': article.get('topic', \"No Topic\"),\n",
    "        'url': article.get('url', \"No URL\")\n",
    "    } for article in most_relevant_articles]\n",
    "\n",
    "    return {\n",
    "        'relevant_topics': relevant_topics,\n",
    "        'combined_summary': combined_summary,\n",
    "        'answers': answers\n",
    "    }\n",
    "\n",
    "# Load preprocessed data\n",
    "preprocessed_data = load_preprocessed_data(\"preprocessed_data.json\")\n",
    "\n",
    "@app.route('/chat', methods=['POST'])\n",
    "def chat():\n",
    "    user_input = request.json.get(\"query\", \"\").strip().lower()\n",
    "    if any(remark in user_input for remark in leaving_remarks):\n",
    "        return jsonify({\"response\": \"Goodbye! Have a great day!\"})\n",
    "    \n",
    "    query_type = classify_query_type(user_input, zero_shot_classifier, query_labels, threshold=0.5)\n",
    "    if query_type[\"label\"] == \"chitchat\":\n",
    "        response = chat_with_blenderbot(user_input)\n",
    "        return jsonify({\"response\": response})\n",
    "    elif query_type[\"label\"] == \"wiki_query\":\n",
    "        try:\n",
    "            result = wiki_qa_system(user_input, preprocessed_data, top_n=3)\n",
    "            return jsonify({\n",
    "                \"relevant_topics\": result[\"relevant_topics\"],\n",
    "                \"summary\": result[\"combined_summary\"],\n",
    "                \"articles\": result[\"answers\"]\n",
    "            })\n",
    "        except ValueError as ve:\n",
    "            return jsonify({\"error\": str(ve)})\n",
    "    else:\n",
    "        return jsonify({\"response\": \"The query could not be classified confidently. Please try again.\"})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True)\n",
    "\n",
    "\n",
    "# In[ ]:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
