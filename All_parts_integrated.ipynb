{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7088d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anirudh/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "[nltk_data] Downloading package punkt to /Users/anirudh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/anirudh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/anirudh/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data loaded successfully.\n",
      "Enter your query below (type 'exit' to quit):\n",
      "Enter query: Health effects of swimming\n",
      "An unexpected error occurred: string indices must be integers, not 'str'\n",
      "Preprocessed data loaded successfully.\n",
      "Enter your query below (type 'exit' to quit):\n",
      "Enter query: Hello\n",
      "BlenderBot:  Hi, how are you? I just got back from walking my dog. Do you have any pets?\n",
      "Enter query: Health\n",
      "An unexpected error occurred: string indices must be integers, not 'str'\n"
     ]
    }
   ],
   "source": [
    "# import json\n",
    "# import numpy as np\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "# from transformers import T5ForConditionalGeneration, T5Tokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# # Download necessary NLTK resources\n",
    "# nltk.download(\"punkt\")\n",
    "# nltk.download(\"stopwords\")\n",
    "# nltk.download(\"wordnet\")\n",
    "\n",
    "# # Load the query classifier\n",
    "# zero_shot_classifier = pipeline(\"zero-shot-classification\", model=\"valhalla/distilbart-mnli-12-1\")\n",
    "# query_labels = [\"chitchat\", \"wiki_query\"]\n",
    "# query_hypothesis_template = \"This is a {}.\"\n",
    "\n",
    "# # Load the chitchat model\n",
    "# blenderbot_tokenizer = AutoTokenizer.from_pretrained(\"facebook/blenderbot-400M-distill\")\n",
    "# blenderbot_model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/blenderbot-400M-distill\")\n",
    "\n",
    "# # Load the T5 model for summarization\n",
    "# t5_model_name = \"t5-base\"\n",
    "# t5_tokenizer = T5Tokenizer.from_pretrained(t5_model_name)\n",
    "# t5_model = T5ForConditionalGeneration.from_pretrained(t5_model_name)\n",
    "\n",
    "# # Reload the zero-shot classification model and tokenizer for topic classification\n",
    "# load_directory = \"./saved_zero_shot_model\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(load_directory)\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(load_directory)\n",
    "# pipe = pipeline(\"zero-shot-classification\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# # Define topics and hypothesis template for topic classification\n",
    "# topics = [\n",
    "#     \"Health\",\n",
    "#     \"Environment\",\n",
    "#     \"Technology\",\n",
    "#     \"Economy\",\n",
    "#     \"Entertainment\",\n",
    "#     \"Sports\",\n",
    "#     \"Politics\",\n",
    "#     \"Education\",\n",
    "#     \"Travel\",\n",
    "#     \"Food\",\n",
    "# ]\n",
    "# topic_hypothesis_template = \"This query is related to {}.\"\n",
    "\n",
    "# # Functions\n",
    "# def classify_query_type(query, classifier, labels, threshold=0.5):\n",
    "#     \"\"\"\n",
    "#     Classifies the query as either 'chitchat' or 'wiki_query'.\n",
    "#     \"\"\"\n",
    "#     output = classifier(query, labels, hypothesis_template=query_hypothesis_template)\n",
    "#     scores = output[\"scores\"]\n",
    "#     best_label = output[\"labels\"][0]\n",
    "#     best_score = scores[0]\n",
    "#     return {\"label\": best_label, \"score\": best_score} if best_score > threshold else {\"label\": \"uncertain\", \"score\": best_score}\n",
    "\n",
    "# def chat_with_blenderbot(input_text):\n",
    "#     \"\"\"\n",
    "#     Handles chitchat using BlenderBot.\n",
    "#     \"\"\"\n",
    "#     inputs = blenderbot_tokenizer(input_text, return_tensors=\"pt\")\n",
    "#     outputs = blenderbot_model.generate(**inputs)\n",
    "#     return blenderbot_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# def classify_multi_topics(query, pipe, topics, threshold=0.1, top_n=5):\n",
    "#     \"\"\"\n",
    "#     Classifies the query into multiple topics using zero-shot classification.\n",
    "#     \"\"\"\n",
    "#     output = pipe(query, topics, hypothesis_template=topic_hypothesis_template)\n",
    "#     labels = output[\"labels\"]\n",
    "#     scores = output[\"scores\"]\n",
    "#     return [label for label, score in zip(labels, scores) if score > threshold][:top_n]\n",
    "\n",
    "# # Preprocess text function\n",
    "# def preprocess_text(text):\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "#     stop_words = set(stopwords.words(\"english\"))\n",
    "#     words = word_tokenize(text.lower())\n",
    "#     filtered_words = [\n",
    "#         lemmatizer.lemmatize(word) for word in words if word.isalnum() and word not in stop_words\n",
    "#     ]\n",
    "#     return \" \".join(filtered_words)\n",
    "\n",
    "# # Load preprocessed data function\n",
    "# def load_preprocessed_data(filename):\n",
    "#     with open(filename, 'r', encoding='utf-8') as file:\n",
    "#         return json.load(file)\n",
    "\n",
    "# # Create a TF-IDF vectorizer function\n",
    "# def create_tfidf_vectorizer(texts):\n",
    "#     vectorizer = TfidfVectorizer()\n",
    "#     tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "#     return vectorizer, tfidf_matrix\n",
    "\n",
    "# # Retrieve most relevant articles with unique URLs\n",
    "# def get_most_relevant_articles(query, tfidf_vectorizer, tfidf_matrix, articles, top_n=3):\n",
    "#     query_vec = tfidf_vectorizer.transform([query])\n",
    "#     cosine_similarities = np.dot(tfidf_matrix, query_vec.T).toarray().flatten()\n",
    "#     sorted_indices = cosine_similarities.argsort()[::-1]\n",
    "\n",
    "#     unique_articles = []\n",
    "#     seen_urls = set()\n",
    "\n",
    "#     for idx in sorted_indices:\n",
    "#         if len(unique_articles) >= top_n:\n",
    "#             break\n",
    "#         article = articles[idx]\n",
    "#         article_url = article.get('url', None)\n",
    "#         if article_url and article_url not in seen_urls:\n",
    "#             unique_articles.append(article)\n",
    "#             seen_urls.add(article_url)\n",
    "\n",
    "#     return unique_articles\n",
    "\n",
    "# # Generate a meaningful summary using T5\n",
    "# def generate_meaningful_summary_t5(combined_text, max_length=800, min_length=100):\n",
    "#     inputs = t5_tokenizer.encode(\"summarize: \" + combined_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "#     summary_ids = t5_model.generate(\n",
    "#         inputs,\n",
    "#         max_length=max_length,\n",
    "#         min_length=min_length,\n",
    "#         length_penalty=1.0,\n",
    "#         num_beams=4,\n",
    "#         early_stopping=True\n",
    "#     )\n",
    "#     return t5_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# # Combine summaries of the most relevant articles\n",
    "# def combine_summaries(articles):\n",
    "#     combined_text = \" \".join([\n",
    "#         article['summary']['text_en'] if 'summary' in article and 'text_en' in article['summary'] else \"No Summary\"\n",
    "#         for article in articles\n",
    "#     ])\n",
    "#     return generate_meaningful_summary_t5(combined_text)\n",
    "\n",
    "# # Main QA function\n",
    "# def wiki_qa_system(query, preprocessed_data, top_n=3):  # Set top_n to 5\n",
    "#     \"\"\"\n",
    "#     Main Wiki QA system function.\n",
    "#     \"\"\"\n",
    "#     relevant_topics_data = classify_multi_topics(query, pipe, topics, threshold=0.1, top_n=5)\n",
    "#     relevant_topics = [t[\"topic\"] for t in relevant_topics_data]\n",
    "#     print(f\"Relevant Topics: {relevant_topics}\")\n",
    "\n",
    "#     all_articles = []\n",
    "#     for topic, data in preprocessed_data.items():\n",
    "#         if topic in relevant_topics and 'articles' in data and isinstance(data['articles'], list):\n",
    "#             for article in data['articles']:\n",
    "#                 if isinstance(article, dict) and 'preprocessed_summary' in article:\n",
    "#                     article['topic'] = topic\n",
    "#                     all_articles.append(article)\n",
    "\n",
    "#     if not all_articles:\n",
    "#         raise ValueError(\"No valid articles found for the relevant topics.\")\n",
    "\n",
    "#     articles_summaries = [\n",
    "#         preprocess_text(article['preprocessed_summary']) for article in all_articles\n",
    "#         if 'preprocessed_summary' in article\n",
    "#     ]\n",
    "\n",
    "#     if not articles_summaries:\n",
    "#         raise ValueError(\"No preprocessed summaries found for matching.\")\n",
    "\n",
    "#     tfidf_vectorizer, tfidf_matrix = create_tfidf_vectorizer(articles_summaries)\n",
    "#     most_relevant_articles = get_most_relevant_articles(query, tfidf_vectorizer, tfidf_matrix, all_articles, top_n)\n",
    "#     combined_summary = combine_summaries(most_relevant_articles)\n",
    "\n",
    "#     answers = [{\n",
    "#         'title': article.get('title', \"No Title\"),\n",
    "#         'topic': article.get('topic', \"No Topic\"),\n",
    "#         'url': article.get('url', \"No URL\")\n",
    "#     } for article in most_relevant_articles]\n",
    "\n",
    "#     return {\n",
    "#         'combined_summary': combined_summary,\n",
    "#         'answers': answers\n",
    "#     }\n",
    "\n",
    "# # Integration of systems\n",
    "# if __name__ == \"__main__\":\n",
    "#     preprocessed_filename = 'preprocessed_data.json'\n",
    "#     try:\n",
    "#         # Load preprocessed data for wiki QA\n",
    "#         preprocessed_data = load_preprocessed_data(preprocessed_filename)\n",
    "#         print(\"Preprocessed data loaded successfully.\")\n",
    "        \n",
    "#         print(\"Enter your query below (type 'exit' to quit):\")\n",
    "#         while True:\n",
    "#             query = input(\"Enter query: \")\n",
    "#             if query.lower() == \"exit\":\n",
    "#                 print(\"Exiting...\")\n",
    "#                 break\n",
    "            \n",
    "#             # Classify the query type\n",
    "#             query_type = classify_query_type(query, zero_shot_classifier, query_labels, threshold=0.5)\n",
    "#             if query_type[\"label\"] == \"chitchat\":\n",
    "#                 # Handle chitchat\n",
    "#                 response = chat_with_blenderbot(query)\n",
    "#                 print(f\"BlenderBot: {response}\")\n",
    "#             elif query_type[\"label\"] == \"wiki_query\":\n",
    "#                 # Handle wiki query\n",
    "#                 result = wiki_qa_system(query, preprocessed_data, top_n=3)\n",
    "#                 print(\"\\nSummary:\")\n",
    "#                 print(result['combined_summary'])\n",
    "#                 print(\"\\nRelevant Articles:\")\n",
    "#                 for answer in result['answers']:\n",
    "#                     print(f\"Title: {answer['title']}\")\n",
    "#                     print(f\"Topic: {answer['topic']}\")\n",
    "#                     print(f\"URL: {answer['url']}\")\n",
    "#                     print()\n",
    "#             else:\n",
    "#                 print(\"The query could not be classified confidently. Please try again.\")\n",
    "#     except ValueError as ve:\n",
    "#         print(f\"Value Error: {ve}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "\n",
    "# # Integration of systems\n",
    "# if __name__ == \"__main__\":\n",
    "#     preprocessed_filename = 'preprocessed_data.json'\n",
    "#     try:\n",
    "#         # Load preprocessed data for wiki QA\n",
    "#         preprocessed_data = load_preprocessed_data(preprocessed_filename)\n",
    "#         print(\"Preprocessed data loaded successfully.\")\n",
    "        \n",
    "#         print(\"Enter your query below (type 'exit' to quit):\")\n",
    "#         while True:\n",
    "#             query = input(\"Enter query: \")\n",
    "#             if query.lower() == \"exit\":\n",
    "#                 print(\"Exiting...\")\n",
    "#                 break\n",
    "            \n",
    "#             # Classify the query type\n",
    "#             query_type = classify_query_type(query, zero_shot_classifier, query_labels, threshold=0.5)\n",
    "#             if query_type[\"label\"] == \"chitchat\":\n",
    "#                 # Handle chitchat\n",
    "#                 response = chat_with_blenderbot(query)\n",
    "#                 print(f\"BlenderBot: {response}\")\n",
    "#             elif query_type[\"label\"] == \"wiki_query\":\n",
    "#                 # Handle wiki query\n",
    "#                 result = wiki_qa_system(query, preprocessed_data, top_n=5)\n",
    "#                 print(\"\\nSummary:\")\n",
    "#                 print(result[\"summary\"])\n",
    "#                 print(\"\\nRelevant Articles:\")\n",
    "#                 for answer in result[\"answers\"]:\n",
    "#                     print(f\"Title: {answer['title']}\")\n",
    "#                     print(f\"Topic: {answer['topic']}\")\n",
    "#                     print(f\"URL: {answer['url']}\")\n",
    "#                     print()\n",
    "#             else:\n",
    "#                 print(\"The query could not be classified confidently. Please try again.\")\n",
    "#     except ValueError as ve:\n",
    "#         print(f\"Value Error: {ve}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89ae2621",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/anirudh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/anirudh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/anirudh/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data loaded successfully.\n",
      "Enter your query below (type 'exit' to quit):\n",
      "Enter query: How does technology impact the environment?\n",
      "Relevant Topics: Environment, Technology\n",
      "\n",
      "Summary:\n",
      "emerging technologies are those technologies whose development practical applications or both are still largely unrealized Emerging technologies are characterized by radical novelty in application even if not in origins relatively fast growth coherence prominent impact and uncertainty and ambiguity Emerging technologies include a variety of technologies such as educational technology information technology nanotechnology biotechnology robotics and artificial intelligence Emerging technologies include a variety of technologies such as educational technology information technology and nanotechnology . the most prominent impact however lies in the future and so in the emergence phase is still\n",
      "\n",
      "Relevant Articles:\n",
      "Title: Emerging technologies\n",
      "Topic: Technology\n",
      "URL: https://en.wikipedia.org/wiki/Emerging_technologies\n",
      "Title: Technology and society\n",
      "Topic: Technology\n",
      "URL: https://en.wikipedia.org/wiki/Technology_and_society\n",
      "Title: Future-oriented technology analysis\n",
      "Topic: Technology\n",
      "URL: https://en.wikipedia.org/wiki/Future-oriented_technology_analysis\n",
      "Enter query: Health benefits of swimming?\n",
      "Relevant Topics: Health, Sports\n",
      "\n",
      "Summary:\n",
      "swimming involves repeated motions known as strokes to propel the body forward while the front crawl is widely regarded as the fastest of the four main strokes . swimmers may find themselves incapacitated by panic and exhaustion both potential causes of death by drowning . health human resources (hrh) is defined as all people engaged in actions whose primary intent is to enhance positive health outcomes according to the world health organizations world health report 2006 . the field of HHR deals with issues such as workforce planning and policy evaluation recruitment\n",
      "\n",
      "Relevant Articles:\n",
      "Title: Swimming\n",
      "Topic: Health\n",
      "URL: https://en.wikipedia.org/wiki/Swimming\n",
      "Title: Sporting CP (swimming)\n",
      "Topic: Sports\n",
      "URL: https://en.wikipedia.org/wiki/Sporting_CP_(swimming)\n",
      "Title: Health human resources\n",
      "Topic: Health\n",
      "URL: https://en.wikipedia.org/wiki/Health_human_resources\n",
      "Enter query: How is your day going?\n",
      "BlenderBot:  It is going well, thank you. How about your day? What are you up to?\n",
      "Enter query: What's your destination for a vacation?\n",
      "BlenderBot:  A cruise to the Bahamas.  I can't wait to go!\n",
      "Enter query: see you\n",
      "Goodbye! Have a great day!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "# Load the query classifier\n",
    "zero_shot_classifier = pipeline(\"zero-shot-classification\", model=\"valhalla/distilbart-mnli-12-1\")\n",
    "query_labels = [\"chitchat\", \"wiki_query\"]\n",
    "query_hypothesis_template = \"This is a {}.\"\n",
    "\n",
    "# Load the chitchat model\n",
    "blenderbot_tokenizer = AutoTokenizer.from_pretrained(\"facebook/blenderbot-400M-distill\")\n",
    "blenderbot_model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/blenderbot-400M-distill\")\n",
    "\n",
    "# Load the T5 model for summarization\n",
    "t5_model_name = \"t5-base\"\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(t5_model_name)\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(t5_model_name)\n",
    "\n",
    "# Reload the zero-shot classification model and tokenizer for topic classification\n",
    "load_directory = \"./saved_zero_shot_model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(load_directory)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(load_directory)\n",
    "pipe = pipeline(\"zero-shot-classification\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Define topics and hypothesis template for topic classification\n",
    "topics = [\n",
    "    \"Health\",\n",
    "    \"Environment\",\n",
    "    \"Technology\",\n",
    "    \"Economy\",\n",
    "    \"Entertainment\",\n",
    "    \"Sports\",\n",
    "    \"Politics\",\n",
    "    \"Education\",\n",
    "    \"Travel\",\n",
    "    \"Food\",\n",
    "]\n",
    "topic_hypothesis_template = \"This query is related to {}.\"\n",
    "\n",
    "# Functions\n",
    "def classify_query_type(query, classifier, labels, threshold=0.5):\n",
    "    output = classifier(query, labels, hypothesis_template=query_hypothesis_template)\n",
    "    scores = output[\"scores\"]\n",
    "    best_label = output[\"labels\"][0]\n",
    "    best_score = scores[0]\n",
    "    return {\"label\": best_label, \"score\": best_score} if best_score > threshold else {\"label\": \"uncertain\", \"score\": best_score}\n",
    "\n",
    "def chat_with_blenderbot(input_text):\n",
    "    inputs = blenderbot_tokenizer(input_text, return_tensors=\"pt\")\n",
    "    outputs = blenderbot_model.generate(**inputs)\n",
    "    return blenderbot_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "def classify_multi_topics(query, pipe, topics, threshold=0.1, top_n=5):\n",
    "    output = pipe(query, topics, hypothesis_template=topic_hypothesis_template)\n",
    "    labels = output[\"labels\"]\n",
    "    scores = output[\"scores\"]\n",
    "    return [label for label, score in zip(labels, scores) if score > threshold][:top_n]\n",
    "\n",
    "def preprocess_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    words = word_tokenize(text.lower())\n",
    "    filtered_words = [\n",
    "        lemmatizer.lemmatize(word) for word in words if word.isalnum() and word not in stop_words\n",
    "    ]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "def load_preprocessed_data(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def create_tfidf_vectorizer(texts):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    return vectorizer, tfidf_matrix\n",
    "\n",
    "def get_most_relevant_articles(query, tfidf_vectorizer, tfidf_matrix, articles, top_n=3):\n",
    "    query_vec = tfidf_vectorizer.transform([query])\n",
    "    cosine_similarities = np.dot(tfidf_matrix, query_vec.T).toarray().flatten()\n",
    "    sorted_indices = cosine_similarities.argsort()[::-1]\n",
    "    unique_articles = []\n",
    "    seen_urls = set()\n",
    "    for idx in sorted_indices:\n",
    "        if len(unique_articles) >= top_n:\n",
    "            break\n",
    "        article = articles[idx]\n",
    "        article_url = article.get('url', None)\n",
    "        if article_url and article_url not in seen_urls:\n",
    "            unique_articles.append(article)\n",
    "            seen_urls.add(article_url)\n",
    "    return unique_articles\n",
    "\n",
    "# Generate a meaningful summary using T5\n",
    "def generate_meaningful_summary_t5(combined_text, max_length=800, min_length=100):\n",
    "    \"\"\"\n",
    "    Generates a meaningful summary using the T5 model.\n",
    "\n",
    "    Args:\n",
    "    - combined_text (str): The combined text from multiple summaries.\n",
    "    - max_length (int): The maximum length of the summary.\n",
    "    - min_length (int): The minimum length of the summary.\n",
    "\n",
    "    Returns:\n",
    "    - str: The generated summary.\n",
    "    \"\"\"\n",
    "    inputs = t5_tokenizer.encode(\"summarize: \" + combined_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    summary_ids = t5_model.generate(\n",
    "        inputs,\n",
    "        max_length=max_length,\n",
    "        min_length=min_length,\n",
    "        length_penalty=1.0,\n",
    "        num_beams=4,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    return t5_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# Combine summaries of the most relevant articles\n",
    "def combine_summaries(articles):\n",
    "    \"\"\"\n",
    "    Combines the summaries of the most relevant articles and generates a meaningful summary.\n",
    "\n",
    "    Args:\n",
    "    - articles (list): A list of articles with their summaries.\n",
    "\n",
    "    Returns:\n",
    "    - str: A combined and summarized text from the articles.\n",
    "    \"\"\"\n",
    "    combined_text = \" \".join([\n",
    "        article['summary']['text_en'] if 'summary' in article and 'text_en' in article['summary'] else \"No Summary\"\n",
    "        for article in articles\n",
    "    ])\n",
    "    \n",
    "    if not combined_text.strip():\n",
    "        return \"No valid summaries available to generate a meaningful summary.\"\n",
    "    \n",
    "    # Generate meaningful summary using T5\n",
    "    return generate_meaningful_summary_t5(combined_text)\n",
    "\n",
    "\n",
    "def wiki_qa_system(query, preprocessed_data, top_n=3):\n",
    "    \"\"\"\n",
    "    Main Wiki QA system function to retrieve relevant articles and generate a summary.\n",
    "\n",
    "    Args:\n",
    "    - query (str): The user's query.\n",
    "    - preprocessed_data (dict): The preprocessed data containing article information.\n",
    "    - top_n (int): The number of top relevant articles to retrieve.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary containing the combined summary and details of relevant articles.\n",
    "    \"\"\"\n",
    "    relevant_topics = classify_multi_topics(query, pipe, topics, threshold=0.1, top_n=5)\n",
    "    all_articles = []\n",
    "    for topic, data in preprocessed_data.items():\n",
    "        if topic in relevant_topics and 'articles' in data and isinstance(data['articles'], list):\n",
    "            for article in data['articles']:\n",
    "                if isinstance(article, dict) and 'preprocessed_summary' in article:\n",
    "                    article['topic'] = topic\n",
    "                    all_articles.append(article)\n",
    "    \n",
    "    if not all_articles:\n",
    "        raise ValueError(\"No valid articles found for the relevant topics.\")\n",
    "\n",
    "    articles_summaries = [\n",
    "        preprocess_text(article['preprocessed_summary']) for article in all_articles\n",
    "        if 'preprocessed_summary' in article\n",
    "    ]\n",
    "    \n",
    "    if not articles_summaries:\n",
    "        raise ValueError(\"No preprocessed summaries found for matching.\")\n",
    "\n",
    "    tfidf_vectorizer, tfidf_matrix = create_tfidf_vectorizer(articles_summaries)\n",
    "    most_relevant_articles = get_most_relevant_articles(query, tfidf_vectorizer, tfidf_matrix, all_articles, top_n)\n",
    "    combined_summary = combine_summaries(most_relevant_articles)\n",
    "    \n",
    "    answers = [{\n",
    "        'title': article.get('title', \"No Title\"),\n",
    "        'topic': article.get('topic', \"No Topic\"),\n",
    "        'url': article.get('url', \"No URL\")\n",
    "    } for article in most_relevant_articles]\n",
    "\n",
    "    return {\n",
    "        'combined_summary': combined_summary,\n",
    "        'answers': answers\n",
    "    }\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    preprocessed_filename = 'preprocessed_data.json'\n",
    "    leaving_remarks = [\"bye\", \"goodbye\", \"exit\", \"see you\", \"later\", \"quit\"]  # Define leaving remarks\n",
    "\n",
    "    try:\n",
    "        preprocessed_data = load_preprocessed_data(preprocessed_filename)\n",
    "        print(\"Preprocessed data loaded successfully.\")\n",
    "        print(\"Enter your query below (type 'exit' to quit):\")\n",
    "\n",
    "        while True:\n",
    "            query = input(\"Enter query: \").strip().lower()  # Normalize query to lowercase\n",
    "            if any(remark in query for remark in leaving_remarks):  # Check for leaving remarks\n",
    "                print(\"Goodbye! Have a great day!\")\n",
    "                break\n",
    "\n",
    "            query_type = classify_query_type(query, zero_shot_classifier, query_labels, threshold=0.5)\n",
    "            if query_type[\"label\"] == \"chitchat\":\n",
    "                response = chat_with_blenderbot(query)\n",
    "                print(f\"BlenderBot: {response}\")\n",
    "            elif query_type[\"label\"] == \"wiki_query\":\n",
    "                try:\n",
    "                    # Get relevant topics\n",
    "                    relevant_topics = classify_multi_topics(query, pipe, topics, threshold=0.1, top_n=5)\n",
    "                    print(f\"Relevant Topics: {', '.join(relevant_topics)}\")\n",
    "\n",
    "                    # Proceed with wiki QA system\n",
    "                    result = wiki_qa_system(query, preprocessed_data, top_n=3)\n",
    "                    print(\"\\nSummary:\")\n",
    "                    print(result[\"combined_summary\"])\n",
    "                    print(\"\\nRelevant Articles:\")\n",
    "                    for answer in result[\"answers\"]:\n",
    "                        print(f\"Title: {answer['title']}\")\n",
    "                        print(f\"Topic: {answer['topic']}\")\n",
    "                        print(f\"URL: {answer['url']}\")\n",
    "                except ValueError as ve:\n",
    "                    print(f\"Error: {ve}\")\n",
    "            else:\n",
    "                print(\"The query could not be classified confidently. Please try again.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e9de9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
