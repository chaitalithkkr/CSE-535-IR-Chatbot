{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Function to load the preprocessed data from a file\n",
    "def load_preprocessed_data(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "# Function to create a TF-IDF vectorizer\n",
    "def create_tfidf_vectorizer(texts):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    return vectorizer, tfidf_matrix\n",
    "\n",
    "# Function to get the most relevant articles based on the query\n",
    "def get_most_relevant_articles(query, tfidf_vectorizer, tfidf_matrix, articles, top_n=3):\n",
    "    query_vec = tfidf_vectorizer.transform([query])  # Transform the query using the vectorizer\n",
    "    cosine_similarities = np.dot(tfidf_matrix, query_vec.T).toarray()  # Compute cosine similarities\n",
    "    \n",
    "    # Get the indices of the top N most relevant articles\n",
    "    best_match_indices = cosine_similarities.flatten().argsort()[-top_n:][::-1]\n",
    "    return [articles[idx] for idx in best_match_indices]  # Return the top N relevant articles\n",
    "\n",
    "# Function to combine summaries of multiple articles\n",
    "def combine_summaries(articles):\n",
    "    combined_summary = \" \".join([article['summary']['text_en'] for article in articles])\n",
    "    return combined_summary\n",
    "\n",
    "# Main QA function\n",
    "def wiki_qa_system(query, preprocessed_data, top_n=3):\n",
    "    all_articles = []\n",
    "    \n",
    "    # Flatten the articles from all topics\n",
    "    for topic, data in preprocessed_data.items():\n",
    "        if 'articles' in data and isinstance(data['articles'], list):  # Check for 'articles' key\n",
    "            for article in data['articles']:\n",
    "                if isinstance(article, dict) and 'preprocessed_summary' in article:\n",
    "                    all_articles.append(article)\n",
    "        else:\n",
    "            print(f\"Unexpected format for topic: {topic}\")\n",
    "\n",
    "    # Ensure all_articles is populated\n",
    "    if not all_articles:\n",
    "        raise ValueError(\"No valid articles found in the preprocessed data.\")\n",
    "\n",
    "    # Use preprocessed summaries for matching\n",
    "    articles_summaries = [article['preprocessed_summary'] for article in all_articles]\n",
    "    tfidf_vectorizer, tfidf_matrix = create_tfidf_vectorizer(articles_summaries)\n",
    "\n",
    "    # Get the top N most relevant articles based on the query\n",
    "    most_relevant_articles = get_most_relevant_articles(query, tfidf_vectorizer, tfidf_matrix, all_articles, top_n)\n",
    "\n",
    "    # Combine the summaries of the most relevant articles\n",
    "    combined_summary = combine_summaries(most_relevant_articles)\n",
    "\n",
    "    # Return the combined summary and the relevant article titles and URLs\n",
    "    answers = [{\n",
    "        'title': article['title'],\n",
    "        'summary': article['summary']['text_en'],\n",
    "        'url': article['url']\n",
    "    } for article in most_relevant_articles]\n",
    "\n",
    "    return {\n",
    "        'combined_summary': combined_summary,\n",
    "        'answers': answers\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    preprocessed_filename = 'preprocessed_data.json'  # Your preprocessed data file\n",
    "    \n",
    "    try:\n",
    "        # Load preprocessed data\n",
    "        preprocessed_data = load_preprocessed_data(preprocessed_filename)\n",
    "        \n",
    "        # Debugging: Check structure\n",
    "        print(\"Preprocessed data loaded successfully.\")\n",
    "        print(\"Sample data structure:\")\n",
    "        print(json.dumps(preprocessed_data, indent=2)[:500])  # Print the first 500 characters for inspection\n",
    "\n",
    "        # Accept user input for the query\n",
    "        query = input(\"Please enter your query: \")\n",
    "\n",
    "        # Get the result from the wiki QA system\n",
    "        result = wiki_qa_system(query, preprocessed_data, top_n=3)\n",
    "\n",
    "        # Display results\n",
    "        print(\"\\nCombined Summary:\")\n",
    "        print(result['combined_summary'])\n",
    "        print(\"\\nRelevant Articles:\")\n",
    "        for answer in result['answers']:\n",
    "            print(f\"Title: {answer['title']}\")\n",
    "            print(f\"Summary: {answer['summary']}\")\n",
    "            print(f\"URL: {answer['url']}\")\n",
    "            print()\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
