{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f447e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anirudh/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "[nltk_data] Downloading package punkt to /Users/anirudh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/anirudh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/anirudh/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1863eaf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/anirudh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/anirudh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/anirudh/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data loaded successfully.\n",
      "Please enter your query: Health benefits of swimming?\n",
      "Query: Health benefits of swimming?\n",
      "Classification Scores:\n",
      "  - Health: 0.61\n",
      "  - Sports: 0.36\n",
      "  - Education: 0.01\n",
      "  - Technology: 0.00\n",
      "  - Environment: 0.00\n",
      "  - Entertainment: 0.00\n",
      "  - Food: 0.00\n",
      "  - Politics: 0.00\n",
      "  - Travel: 0.00\n",
      "  - Economy: 0.00\n",
      "Relevant Topics: ['Health', 'Sports']\n",
      "\n",
      "Summary:\n",
      "swimming involves repeated motions known as strokes to propel the body forward while the front crawl is widely regarded as the fastest of the four main strokes . swimmers may find themselves incapacitated by panic and exhaustion both potential causes of death by drowning . health human resources (hrh) is defined as all people engaged in actions whose primary intent is to enhance positive health outcomes according to the world health organizations world health report 2006 . the field of HHR deals with issues such as workforce planning and policy evaluation recruitment\n",
      "\n",
      "Relevant Articles:\n",
      "Title: Swimming\n",
      "Topic: Health\n",
      "URL: https://en.wikipedia.org/wiki/Swimming\n",
      "\n",
      "Title: Sporting CP (swimming)\n",
      "Topic: Sports\n",
      "URL: https://en.wikipedia.org/wiki/Sporting_CP_(swimming)\n",
      "\n",
      "Title: Health human resources\n",
      "Topic: Health\n",
      "URL: https://en.wikipedia.org/wiki/Health_human_resources\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "# Load the T5 model for summarization\n",
    "t5_model_name = \"t5-base\"  # You can also use \"t5-large\" for better performance\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(t5_model_name)\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(t5_model_name)\n",
    "\n",
    "# Reload the zero-shot classification model and tokenizer\n",
    "load_directory = \"./saved_zero_shot_model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(load_directory)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(load_directory)\n",
    "\n",
    "# Create a classification pipeline with the loaded model\n",
    "pipe = pipeline(\"zero-shot-classification\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Define the topics\n",
    "topics = [\n",
    "    \"Health\",\n",
    "    \"Environment\",\n",
    "    \"Technology\",\n",
    "    \"Economy\",\n",
    "    \"Entertainment\",\n",
    "    \"Sports\",\n",
    "    \"Politics\",\n",
    "    \"Education\",\n",
    "    \"Travel\",\n",
    "    \"Food\",\n",
    "]\n",
    "\n",
    "# Define the hypothesis template\n",
    "hypothesis_template = \"This query is related to {}.\"\n",
    "\n",
    "# Improved topic classification function\n",
    "def classify_multi_topics(query, pipe, topics, threshold=0.1, top_n=5):\n",
    "    \"\"\"\n",
    "    Classifies the query into multiple topics using zero-shot classification.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Perform zero-shot classification\n",
    "        output = pipe(query, topics, hypothesis_template=hypothesis_template)\n",
    "\n",
    "        # Extract labels and scores\n",
    "        labels = output[\"labels\"]\n",
    "        scores = output[\"scores\"]\n",
    "\n",
    "        # Filter topics based on the threshold\n",
    "        relevant_topics = [\n",
    "            {\"topic\": label, \"score\": score}\n",
    "            for label, score in zip(labels, scores)\n",
    "            if score > threshold\n",
    "        ]\n",
    "\n",
    "        # Sort topics by score in descending order\n",
    "        relevant_topics = sorted(relevant_topics, key=lambda x: x[\"score\"], reverse=True)\n",
    "\n",
    "        # Debugging: Show all topic scores\n",
    "        print(f\"Query: {query}\")\n",
    "        print(f\"Classification Scores:\")\n",
    "        for t in zip(labels, scores):\n",
    "            print(f\"  - {t[0]}: {t[1]:.2f}\")\n",
    "\n",
    "        # Return top_n topics if none pass the threshold\n",
    "        if not relevant_topics:\n",
    "            print(\"No topics passed the threshold. Returning top N topics instead.\")\n",
    "            relevant_topics = [\n",
    "                {\"topic\": label, \"score\": score}\n",
    "                for label, score in zip(labels[:top_n], scores[:top_n])\n",
    "            ]\n",
    "\n",
    "        return relevant_topics\n",
    "    except Exception as e:\n",
    "        print(f\"Error during classification: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "# Preprocess text function\n",
    "def preprocess_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    words = word_tokenize(text.lower())\n",
    "    filtered_words = [\n",
    "        lemmatizer.lemmatize(word) for word in words if word.isalnum() and word not in stop_words\n",
    "    ]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "# Load preprocessed data function\n",
    "def load_preprocessed_data(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "# Create a TF-IDF vectorizer function\n",
    "def create_tfidf_vectorizer(texts):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    return vectorizer, tfidf_matrix\n",
    "\n",
    "# Retrieve most relevant articles with unique URLs\n",
    "def get_most_relevant_articles(query, tfidf_vectorizer, tfidf_matrix, articles, top_n=3):\n",
    "    query_vec = tfidf_vectorizer.transform([query])\n",
    "    cosine_similarities = np.dot(tfidf_matrix, query_vec.T).toarray().flatten()\n",
    "    sorted_indices = cosine_similarities.argsort()[::-1]\n",
    "\n",
    "    unique_articles = []\n",
    "    seen_urls = set()\n",
    "\n",
    "    for idx in sorted_indices:\n",
    "        if len(unique_articles) >= top_n:\n",
    "            break\n",
    "        article = articles[idx]\n",
    "        article_url = article.get('url', None)\n",
    "        if article_url and article_url not in seen_urls:\n",
    "            unique_articles.append(article)\n",
    "            seen_urls.add(article_url)\n",
    "\n",
    "    return unique_articles\n",
    "\n",
    "# Generate a meaningful summary using T5\n",
    "def generate_meaningful_summary_t5(combined_text, max_length=800, min_length=100):\n",
    "    inputs = t5_tokenizer.encode(\"summarize: \" + combined_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    summary_ids = t5_model.generate(\n",
    "        inputs,\n",
    "        max_length=max_length,\n",
    "        min_length=min_length,\n",
    "        length_penalty=1.0,\n",
    "        num_beams=4,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    return t5_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Combine summaries of the most relevant articles\n",
    "def combine_summaries(articles):\n",
    "    combined_text = \" \".join([\n",
    "        article['summary']['text_en'] if 'summary' in article and 'text_en' in article['summary'] else \"No Summary\"\n",
    "        for article in articles\n",
    "    ])\n",
    "    return generate_meaningful_summary_t5(combined_text)\n",
    "\n",
    "# Main QA function\n",
    "def wiki_qa_system(query, preprocessed_data, top_n=3):  # Set top_n to 5\n",
    "    \"\"\"\n",
    "    Main Wiki QA system function.\n",
    "    \"\"\"\n",
    "    relevant_topics_data = classify_multi_topics(query, pipe, topics, threshold=0.1, top_n=5)\n",
    "    relevant_topics = [t[\"topic\"] for t in relevant_topics_data]\n",
    "    print(f\"Relevant Topics: {relevant_topics}\")\n",
    "\n",
    "    all_articles = []\n",
    "    for topic, data in preprocessed_data.items():\n",
    "        if topic in relevant_topics and 'articles' in data and isinstance(data['articles'], list):\n",
    "            for article in data['articles']:\n",
    "                if isinstance(article, dict) and 'preprocessed_summary' in article:\n",
    "                    article['topic'] = topic\n",
    "                    all_articles.append(article)\n",
    "\n",
    "    if not all_articles:\n",
    "        raise ValueError(\"No valid articles found for the relevant topics.\")\n",
    "\n",
    "    articles_summaries = [\n",
    "        preprocess_text(article['preprocessed_summary']) for article in all_articles\n",
    "        if 'preprocessed_summary' in article\n",
    "    ]\n",
    "\n",
    "    if not articles_summaries:\n",
    "        raise ValueError(\"No preprocessed summaries found for matching.\")\n",
    "\n",
    "    tfidf_vectorizer, tfidf_matrix = create_tfidf_vectorizer(articles_summaries)\n",
    "    most_relevant_articles = get_most_relevant_articles(query, tfidf_vectorizer, tfidf_matrix, all_articles, top_n)\n",
    "    combined_summary = combine_summaries(most_relevant_articles)\n",
    "\n",
    "    answers = [{\n",
    "        'title': article.get('title', \"No Title\"),\n",
    "        'topic': article.get('topic', \"No Topic\"),\n",
    "        'url': article.get('url', \"No URL\")\n",
    "    } for article in most_relevant_articles]\n",
    "\n",
    "    return {\n",
    "        'combined_summary': combined_summary,\n",
    "        'answers': answers\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    preprocessed_filename = 'preprocessed_data.json'\n",
    "    try:\n",
    "        preprocessed_data = load_preprocessed_data(preprocessed_filename)\n",
    "        print(\"Preprocessed data loaded successfully.\")\n",
    "        query = input(\"Please enter your query: \")\n",
    "        result = wiki_qa_system(query, preprocessed_data, top_n=3)\n",
    "        print(\"\\nSummary:\")\n",
    "        print(result['combined_summary'])\n",
    "        print(\"\\nRelevant Articles:\")\n",
    "        for answer in result['answers']:\n",
    "            print(f\"Title: {answer['title']}\")\n",
    "            print(f\"Topic: {answer['topic']}\")\n",
    "            print(f\"URL: {answer['url']}\")\n",
    "            print()\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e621cebf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
