{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "181f5f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wikipedia in /Users/anirudh/anaconda3/lib/python3.11/site-packages (1.4.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/anirudh/anaconda3/lib/python3.11/site-packages (from wikipedia) (4.12.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /Users/anirudh/anaconda3/lib/python3.11/site-packages (from wikipedia) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/anirudh/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/anirudh/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/anirudh/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/anirudh/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2023.7.22)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/anirudh/anaconda3/lib/python3.11/site-packages (from beautifulsoup4->wikipedia) (2.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8240afad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/anirudh/anaconda3/lib/python3.11/site-packages (1.5.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/anirudh/anaconda3/lib/python3.11/site-packages (from pandas) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/anirudh/anaconda3/lib/python3.11/site-packages (from pandas) (2022.7)\r\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/anirudh/anaconda3/lib/python3.11/site-packages (from pandas) (1.24.3)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/anirudh/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71f4a4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "import re\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aafa8dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package wikipedia:\n",
      "\n",
      "NAME\n",
      "    wikipedia\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    exceptions\n",
      "    util\n",
      "    wikipedia\n",
      "\n",
      "DATA\n",
      "    API_URL = 'http://en.wikipedia.org/w/api.php'\n",
      "    ODD_ERROR_MESSAGE = \"This shouldn't happen. Please report on GitHub: g...\n",
      "    RATE_LIMIT = False\n",
      "    RATE_LIMIT_LAST_CALL = None\n",
      "    RATE_LIMIT_MIN_WAIT = None\n",
      "    USER_AGENT = 'wikipedia (https://github.com/goldsmith/Wikipedia/)'\n",
      "    geosearch = <wikipedia.util.cache object>\n",
      "        Do a wikipedia geo search for `latitude` and `longitude`\n",
      "        using HTTP API described in http://www.mediawiki.org/wiki/Extension:GeoData\n",
      "        \n",
      "        Arguments:\n",
      "        \n",
      "        * latitude (float or decimal.Decimal)\n",
      "        * longitude (float or decimal.Decimal)\n",
      "        \n",
      "        Keyword arguments:\n",
      "        \n",
      "        * title - The title of an article to search for\n",
      "        * results - the maximum number of results returned\n",
      "        * radius - Search radius in meters. The value must be between 10 and 10000\n",
      "    \n",
      "    languages = <wikipedia.util.cache object>\n",
      "        List all the currently supported language prefixes (usually ISO language code).\n",
      "        \n",
      "        Can be inputted to `set_lang` to change the Mediawiki that `wikipedia` requests\n",
      "        results from.\n",
      "        \n",
      "        Returns: dict of <prefix>: <local_lang_name> pairs. To get just a list of prefixes,\n",
      "        use `wikipedia.languages().keys()`.\n",
      "    \n",
      "    search = <wikipedia.util.cache object>\n",
      "        Do a Wikipedia search for `query`.\n",
      "        \n",
      "        Keyword arguments:\n",
      "        \n",
      "        * results - the maxmimum number of results returned\n",
      "        * suggestion - if True, return results and suggestion (if any) in a tuple\n",
      "    \n",
      "    suggest = <wikipedia.util.cache object>\n",
      "        Get a Wikipedia search suggestion for `query`.\n",
      "        Returns a string or None if no suggestion was found.\n",
      "    \n",
      "    summary = <wikipedia.util.cache object>\n",
      "        Plain text summary of the page.\n",
      "        \n",
      "        .. note:: This is a convenience wrapper - auto_suggest and redirect are enabled by default\n",
      "        \n",
      "        Keyword arguments:\n",
      "        \n",
      "        * sentences - if set, return the first `sentences` sentences (can be no greater than 10).\n",
      "        * chars - if set, return only the first `chars` characters (actual text returned may be slightly longer).\n",
      "        * auto_suggest - let Wikipedia find a valid page title for the query\n",
      "        * redirect - allow redirection without raising RedirectError\n",
      "\n",
      "VERSION\n",
      "    (1, 4, 0)\n",
      "\n",
      "FILE\n",
      "    /Users/anirudh/anaconda3/lib/python3.11/site-packages/wikipedia/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(wikipedia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77c191c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topics Dictionary with their related subtopics\n",
    "topics = {\n",
    "    'Health': ['common diseases', 'global health statistics', 'mental health trends'],\n",
    "    'Environment': ['global warming', 'endangered species', 'deforestation rates'],\n",
    "    'Technology': ['emerging technologies', 'AI advancements'],\n",
    "    'Economy': ['stock market performance', 'job markets', 'cryptocurrency trends'],\n",
    "    'Entertainment': ['music industry', 'popular cultural events', 'streaming platforms'],\n",
    "    'Sports': ['major sporting events', 'sports analytics'],\n",
    "    'Politics': ['elections', 'public policy analysis', 'international relations'],\n",
    "    'Education': ['literacy rates', 'online education trends', 'student loan data'],\n",
    "    'Travel': ['top tourist destinations', 'airline industry data', 'travel trends'],\n",
    "    'Food': ['crop yield statistics', 'global hunger', 'food security']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68816681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(summary):\n",
    "    # Regex for keeping only alphanumeric characters\n",
    "    return re.sub(r'[^a-zA-Z0-9 ]+', '', summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f612572f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wikipedia_data_scrape(main_topic, subtopic, max_docs, min_summary_length, global_visited_url):\n",
    "    visited_url = set()  # To prevent duplication within a single subtopic\n",
    "    unique_titles = set()  # To ensure unique content by title\n",
    "    unique_summaries = set()  # To ensure unique content by summary hash\n",
    "    docs = []\n",
    "    short_summary_count = 0\n",
    "    search_results = wikipedia.search(subtopic, results=500)\n",
    "    \n",
    "    for result in search_results:\n",
    "        if len(docs) >= max_docs:\n",
    "            break        \n",
    "        try:\n",
    "            content = wikipedia.page(result, auto_suggest=False)\n",
    "            if content.url in visited_url or content.url in global_visited_url:\n",
    "                continue\n",
    "            \n",
    "            cleaned_summary = preprocessing(content.summary)\n",
    "            \n",
    "            # Ensure cleaned_summary is not empty\n",
    "            if not cleaned_summary.strip():\n",
    "                continue  # Skip null or empty summaries\n",
    "            \n",
    "            if len(cleaned_summary) < min_summary_length:\n",
    "                short_summary_count += 1\n",
    "                if short_summary_count / max_docs > 0.05:  # Only allow 5% of summaries to be short\n",
    "                    continue\n",
    "            \n",
    "            # Uniqueness check based on title and summary\n",
    "            if content.title in unique_titles or hash(cleaned_summary) in unique_summaries:\n",
    "                continue\n",
    "            \n",
    "            # Append document with correct field types\n",
    "            docs.append({\n",
    "                'title': str(content.title),\n",
    "                'revision_id': str(content.revision_id),  # Ensure revision_id is a string\n",
    "                'summary': {'text_en': cleaned_summary},  # Ensure the field is marked text_en\n",
    "                'url': content.url,\n",
    "                'topic': main_topic\n",
    "            })\n",
    "            \n",
    "            # Update uniqueness sets\n",
    "            unique_titles.add(content.title)\n",
    "            unique_summaries.add(hash(cleaned_summary))\n",
    "            visited_url.add(content.url)\n",
    "            global_visited_url.add(content.url)\n",
    "        \n",
    "            # Process linked pages\n",
    "            if len(docs) < max_docs:\n",
    "                for link in content.links:\n",
    "                    if len(docs) >= max_docs:\n",
    "                        break\n",
    "                    \n",
    "                    try:\n",
    "                        linked_content = wikipedia.page(link, auto_suggest=False)\n",
    "                        if linked_content.url in visited_url or linked_content.url in global_visited_url:\n",
    "                            continue\n",
    "                        \n",
    "                        linked_summary = preprocessing(linked_content.summary)\n",
    "                        \n",
    "                        if not linked_summary.strip():\n",
    "                            continue  # Skip null or empty summaries\n",
    "                        \n",
    "                        if len(linked_summary) < min_summary_length:\n",
    "                            short_summary_count += 1\n",
    "                            if short_summary_count / max_docs > 0.05:\n",
    "                                continue\n",
    "                        \n",
    "                        # Uniqueness check based on title and summary\n",
    "                        if linked_content.title in unique_titles or hash(linked_summary) in unique_summaries:\n",
    "                            continue\n",
    "                        \n",
    "                        docs.append({\n",
    "                            'title': str(linked_content.title),\n",
    "                            'revision_id': str(linked_content.revision_id),\n",
    "                            'summary': {'text_en': linked_summary},\n",
    "                            'url': linked_content.url,\n",
    "                            'topic': main_topic\n",
    "                        })\n",
    "\n",
    "                        unique_titles.add(linked_content.title)\n",
    "                        unique_summaries.add(hash(linked_summary))\n",
    "                        visited_url.add(linked_content.url) \n",
    "                        global_visited_url.add(linked_content.url)\n",
    "                    \n",
    "                    except wikipedia.exceptions.DisambiguationError:\n",
    "                        continue\n",
    "                    except wikipedia.exceptions.PageError:\n",
    "                        continue\n",
    "        except wikipedia.exceptions.DisambiguationError as e:\n",
    "            print(f\"DisambiguationError for {result}: {e}\")\n",
    "        except wikipedia.exceptions.PageError as e:\n",
    "            print(f\"PageError for {result}: {e}\")\n",
    "    \n",
    "    # to ensure all summaries are non-empty\n",
    "    for doc in docs:\n",
    "        assert doc['summary']['text_en'].strip(), f\"Summary is null or empty for revision ID: {doc['revision_id']}\"\n",
    "\n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15b845a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_topics_scrape(topics):\n",
    "    data = {}\n",
    "    \n",
    "    for main_topic, subtopics in topics.items():\n",
    "        data[main_topic] = []  \n",
    "        global_visited_url = set() # to prevent duplication within subtopics under a single topic\n",
    "        for subtopic in subtopics:\n",
    "            print(f\"Fetching started data for subtopic: {subtopic} under topic: {main_topic}\")\n",
    "            documents = wikipedia_data_scrape(main_topic, subtopic, 3000, 200, global_visited_url) # For each subtopic within a single topic this function will scrape 300 docs\n",
    "            data[main_topic].extend(documents)  \n",
    "            print(f\"Fetching of {len(documents)} documents completed for subtopic: {subtopic}\")\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60571576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching started data for subtopic: common diseases under topic: Health\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anirudh/anaconda3/lib/python3.11/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /Users/anirudh/anaconda3/lib/python3.11/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching of 3000 documents completed for subtopic: common diseases\n",
      "Fetching started data for subtopic: global health statistics under topic: Health\n",
      "Fetching of 3000 documents completed for subtopic: global health statistics\n",
      "Fetching started data for subtopic: mental health trends under topic: Health\n",
      "Fetching of 3000 documents completed for subtopic: mental health trends\n",
      "Fetching started data for subtopic: global warming under topic: Environment\n",
      "DisambiguationError for Global warming (disambiguation): \"Global warming (disambiguation)\" may refer to: \n",
      "global surface temperature\n",
      "global surface temperature\n",
      "ocean heat content\n",
      "ocean temperature\n",
      "sea surface temperature\n",
      "Earth's Energy Imbalance\n",
      "Global Warming (Pitbull album)\n",
      "Global Warming (Sonny Rollins album)\n",
      "Global Warming: The Signs and The Science\n",
      "Global Warming: What You Need to Know\n",
      "From Mars to Sirius\n",
      "Continent\n",
      "All pages with titles beginning with Global warming\n",
      "All pages with titles containing Global warming\n",
      "Global Warning (disambiguation)\n",
      "DisambiguationError for Unstoppable global warming: \"Unstoppable global warming\" may refer to: \n",
      "Runaway climate change\n",
      "Unstoppable Global Warming\n",
      "Fetching of 3000 documents completed for subtopic: global warming\n",
      "Fetching started data for subtopic: endangered species under topic: Environment\n",
      "DisambiguationError for Endangered species (disambiguation): \"Endangered species (disambiguation)\" may refer to: \n",
      "Endangered Species (1982 film)\n",
      "Endangered Species (2002 film)\n",
      "Eli Roth\n",
      "Max Steel: Endangered Species\n",
      "Endangered Species (2021 film)\n",
      "Endangered Species (novel)\n",
      "\"Endangered Species\" (The Amazing Spider-Man)\n",
      "X-Men: Endangered Species\n",
      "Gene Wolfe\n",
      "Hardy Boys novel\n",
      "Endangered Species (Big Pun album)\n",
      "Endangered Species (Des'ree album)\n",
      "Endangered Species (Endangered Species album)\n",
      "Endangered Species (eX-Girl album)\n",
      "Endangered Species (Flaw album)\n",
      "Endangered Species (H.A.W.K. album)\n",
      "Endangered Species (Klaatu album)\n",
      "Endangered Species (Lynyrd Skynyrd album)\n",
      "Endangered Species (Man album)\n",
      "Endangered Species (Y&T album)\n",
      "U.K. Subs\n",
      "Black Sun Empire\n",
      "Endangered Species (Tales from the Darkside)\n",
      "Roots\n",
      "Atlantis\n",
      "Bop Gun (Endangered Species)\n",
      "Endangered Species (TV series)\n",
      "Zoo Tycoon 2: Endangered Species\n",
      "Endangered species (IUCN status)\n",
      "Fetching of 3000 documents completed for subtopic: endangered species\n",
      "Fetching started data for subtopic: deforestation rates under topic: Environment\n",
      "Fetching of 3000 documents completed for subtopic: deforestation rates\n",
      "Fetching started data for subtopic: emerging technologies under topic: Technology\n",
      "Fetching of 3000 documents completed for subtopic: emerging technologies\n",
      "Fetching started data for subtopic: AI advancements under topic: Technology\n"
     ]
    },
    {
     "ename": "ConnectTimeout",
     "evalue": "HTTPConnectionPool(host='en.wikipedia.org', port=80): Max retries exceeded with url: /w/api.php?prop=extracts&explaintext=&exintro=&titles=Education+in+Bahrain&format=json&action=query (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x13a1472d0>, 'Connection to en.wikipedia.org timed out. (connect timeout=None)'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connection.py:174\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     conn \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mcreate_connection(\n\u001b[1;32m    175\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dns_host, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_kw\n\u001b[1;32m    176\u001b[0m     )\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/util/connection.py:95\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m socket\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgetaddrinfo returns an empty list\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/util/connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     84\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 85\u001b[0m sock\u001b[38;5;241m.\u001b[39mconnect(sa)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sock\n",
      "\u001b[0;31mTimeoutError\u001b[0m: [Errno 60] Operation timed out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectTimeoutError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:714\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 714\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[1;32m    715\u001b[0m     conn,\n\u001b[1;32m    716\u001b[0m     method,\n\u001b[1;32m    717\u001b[0m     url,\n\u001b[1;32m    718\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[1;32m    719\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    720\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    721\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    722\u001b[0m )\n\u001b[1;32m    724\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:415\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m         conn\u001b[38;5;241m.\u001b[39mrequest(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhttplib_request_kw)\n\u001b[1;32m    417\u001b[0m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connection.py:244\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers)\u001b[0m\n\u001b[1;32m    243\u001b[0m     headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m _get_default_user_agent()\n\u001b[0;32m--> 244\u001b[0m \u001b[38;5;28msuper\u001b[39m(HTTPConnection, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mrequest(method, url, body\u001b[38;5;241m=\u001b[39mbody, headers\u001b[38;5;241m=\u001b[39mheaders)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:1286\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1285\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1286\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_request(method, url, body, headers, encode_chunked)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:1332\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1331\u001b[0m     body \u001b[38;5;241m=\u001b[39m _encode(body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1332\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendheaders(body, encode_chunked\u001b[38;5;241m=\u001b[39mencode_chunked)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:1281\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_output(message_body, encode_chunked\u001b[38;5;241m=\u001b[39mencode_chunked)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:1041\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1041\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(msg)\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1044\u001b[0m \n\u001b[1;32m   1045\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:979\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[0;32m--> 979\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnect()\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connection.py:205\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 205\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_conn()\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_conn(conn)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connection.py:179\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout:\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ConnectTimeoutError(\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    181\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m timed out. (connect timeout=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    182\u001b[0m         \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout),\n\u001b[1;32m    183\u001b[0m     )\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mConnectTimeoutError\u001b[0m: (<urllib3.connection.HTTPConnection object at 0x13a1472d0>, 'Connection to en.wikipedia.org timed out. (connect timeout=None)')",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[1;32m    487\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    488\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m    489\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[1;32m    490\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    491\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    492\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    493\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    494\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    495\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[1;32m    496\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    497\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:798\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    796\u001b[0m     e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n\u001b[0;32m--> 798\u001b[0m retries \u001b[38;5;241m=\u001b[39m retries\u001b[38;5;241m.\u001b[39mincrement(\n\u001b[1;32m    799\u001b[0m     method, url, error\u001b[38;5;241m=\u001b[39me, _pool\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, _stacktrace\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    800\u001b[0m )\n\u001b[1;32m    801\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/util/retry.py:592\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_retry\u001b[38;5;241m.\u001b[39mis_exhausted():\n\u001b[0;32m--> 592\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause))\n\u001b[1;32m    594\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='en.wikipedia.org', port=80): Max retries exceeded with url: /w/api.php?prop=extracts&explaintext=&exintro=&titles=Education+in+Bahrain&format=json&action=query (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x13a1472d0>, 'Connection to en.wikipedia.org timed out. (connect timeout=None)'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectTimeout\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m all_scraped_data \u001b[38;5;241m=\u001b[39m all_topics_scrape(topics)\n",
      "Cell \u001b[0;32mIn[14], line 9\u001b[0m, in \u001b[0;36mall_topics_scrape\u001b[0;34m(topics)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m subtopic \u001b[38;5;129;01min\u001b[39;00m subtopics:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFetching started data for subtopic: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubtopic\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m under topic: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmain_topic\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m     documents \u001b[38;5;241m=\u001b[39m wikipedia_data_scrape(main_topic, subtopic, \u001b[38;5;241m3000\u001b[39m, \u001b[38;5;241m200\u001b[39m, global_visited_url) \u001b[38;5;66;03m# For each subtopic within a single topic this function will scrape 300 docs\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     data[main_topic]\u001b[38;5;241m.\u001b[39mextend(documents)  \n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFetching of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(documents)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m documents completed for subtopic: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubtopic\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[13], line 58\u001b[0m, in \u001b[0;36mwikipedia_data_scrape\u001b[0;34m(main_topic, subtopic, max_docs, min_summary_length, global_visited_url)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m linked_content\u001b[38;5;241m.\u001b[39murl \u001b[38;5;129;01min\u001b[39;00m visited_url \u001b[38;5;129;01mor\u001b[39;00m linked_content\u001b[38;5;241m.\u001b[39murl \u001b[38;5;129;01min\u001b[39;00m global_visited_url:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m linked_summary \u001b[38;5;241m=\u001b[39m preprocessing(linked_content\u001b[38;5;241m.\u001b[39msummary)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m linked_summary\u001b[38;5;241m.\u001b[39mstrip():\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# Skip null or empty summaries\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/wikipedia/wikipedia.py:530\u001b[0m, in \u001b[0;36mWikipediaPage.summary\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    527\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    528\u001b[0m      query_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpageids\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpageid\n\u001b[0;32m--> 530\u001b[0m   request \u001b[38;5;241m=\u001b[39m _wiki_request(query_params)\n\u001b[1;32m    531\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_summary \u001b[38;5;241m=\u001b[39m request[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpages\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpageid][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mextract\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_summary\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/wikipedia/wikipedia.py:737\u001b[0m, in \u001b[0;36m_wiki_request\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m    734\u001b[0m   wait_time \u001b[38;5;241m=\u001b[39m (RATE_LIMIT_LAST_CALL \u001b[38;5;241m+\u001b[39m RATE_LIMIT_MIN_WAIT) \u001b[38;5;241m-\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m    735\u001b[0m   time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mint\u001b[39m(wait_time\u001b[38;5;241m.\u001b[39mtotal_seconds()))\n\u001b[0;32m--> 737\u001b[0m r \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(API_URL, params\u001b[38;5;241m=\u001b[39mparams, headers\u001b[38;5;241m=\u001b[39mheaders)\n\u001b[1;32m    739\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RATE_LIMIT:\n\u001b[1;32m    740\u001b[0m   RATE_LIMIT_LAST_CALL \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/adapters.py:507\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, ConnectTimeoutError):\n\u001b[1;32m    505\u001b[0m     \u001b[38;5;66;03m# TODO: Remove this in 3.0.0: see #2811\u001b[39;00m\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, NewConnectionError):\n\u001b[0;32m--> 507\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ConnectTimeout(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, ResponseError):\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RetryError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[0;31mConnectTimeout\u001b[0m: HTTPConnectionPool(host='en.wikipedia.org', port=80): Max retries exceeded with url: /w/api.php?prop=extracts&explaintext=&exintro=&titles=Education+in+Bahrain&format=json&action=query (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x13a1472d0>, 'Connection to en.wikipedia.org timed out. (connect timeout=None)'))"
     ]
    }
   ],
   "source": [
    "all_scraped_data = all_topics_scrape(topics) # Total 70000 docs will be scraped and preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57628e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wikipedia_scraped_data.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_scraped_data, f, ensure_ascii=False, indent=4)  # saving the data in a json file\n",
    "\n",
    "print(\"Data saved to 'wikipedia_scraped_data.json'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c16c0bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c9712f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
